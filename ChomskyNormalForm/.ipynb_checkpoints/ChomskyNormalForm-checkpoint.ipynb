{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import cv2\n",
    "import re\n",
    "%matplotlib inline\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B has epsilon production\n",
      "A has epsilon production\n",
      "{'SA': 'U_{0}', 'a': 'U_{1}'}\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$A \\rightarrow U_{1}B|AU_{0}|SA|AS|b|a$$\n",
       "$$B \\rightarrow b$$\n",
       "$$S \\rightarrow U_{1}B|AU_{0}|SA|AS|a$$\n",
       "$$S_{0} \\rightarrow U_{1}B|AU_{0}|SA|AS|a$$\n",
       "$$U_{0} \\rightarrow SA$$\n",
       "$$U_{1} \\rightarrow a$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def has_flag(flags, idx):\n",
    "    return flags & (0x1 << (idx))\n",
    "\n",
    "def generate_eps_combinations(var_name, expr):\n",
    "    split = expr.split(sep=var_name)\n",
    "    flags = 0\n",
    "    split_locations = []\n",
    "    combinations = {expr}\n",
    "    \n",
    "    if expr == var_name:\n",
    "        return 'ε'\n",
    "    \n",
    "    for i, v in enumerate(split):\n",
    "        if not v and i < len(split)-1:\n",
    "            split_locations.append(i+1)\n",
    "        elif i < len(split)-1:\n",
    "            split_locations.append(i+1)\n",
    "    \n",
    "    while flags < 2**len(split_locations)-1:\n",
    "        offset = 0\n",
    "        for i, v in enumerate(split_locations):\n",
    "            if has_flag(flags, i):\n",
    "                idx = v + offset\n",
    "                offset = offset + 1\n",
    "                split.insert(idx, var_name)\n",
    "        \n",
    "        combinations.add(''.join(split))\n",
    "        split = [v for v in split if v != var_name]\n",
    "        flags = flags + 1\n",
    "    \n",
    "    combinations.remove(expr)\n",
    "    return list(combinations)\n",
    "\n",
    "match_lowercase = re.compile('''([a-z])''')\n",
    "match_variable = re.compile('''(\\w_\\\\{\\d+\\\\}|[A-Za-z])''')\n",
    "\n",
    "def generate_new_rules(rules, expr):\n",
    "    variables = match_variable.findall(expr)\n",
    "    res = False\n",
    "    \n",
    "    if len(variables) >= 3:\n",
    "        rules.add(''.join(variables[len(variables) - 2:]))\n",
    "        res = True\n",
    "    elif len(variables) == 2 and match_lowercase.search(expr):\n",
    "        rules.add(match_lowercase.findall(expr)[0])\n",
    "        res = True\n",
    "    \n",
    "    return res\n",
    "\n",
    "class ContextFreeGrammar():\n",
    "    PATTERN = '''(\\w_\\\\{\\d+\\\\}|[A-Za-z]+)'''\n",
    "    EPSILON = 'ε'\n",
    "    ALT_EPSILON = 'e'\n",
    "    START_VARIABLE = 'S'\n",
    "    \n",
    "    def __init__(self, start_var=START_VARIABLE):\n",
    "        self._rules = {}\n",
    "        self._variables = []\n",
    "        self._expr = ContextFreeGrammar.PATTERN\n",
    "        self._match_lowercase = re.compile('''([a-z])''')\n",
    "        self._re_expr = re.compile(self._expr)\n",
    "        self._start_var = start_var\n",
    "        self._u_idx = 0\n",
    "    \n",
    "    @property\n",
    "    def rules(self):\n",
    "        return self._rules\n",
    "    \n",
    "    @property\n",
    "    def variables(self):\n",
    "        return self._variables\n",
    "    \n",
    "    @property\n",
    "    def start_variable(self):\n",
    "        return self._start_var\n",
    "    \n",
    "    def add_rule(self, rule, use_alt_eps=True):\n",
    "        for i, match in enumerate(self._re_expr.finditer(rule)):\n",
    "            v = match.group()\n",
    "            \n",
    "            if not i:\n",
    "                self.variables.append(v)\n",
    "                self.rules[v] = []\n",
    "            else:\n",
    "                if v == ContextFreeGrammar.ALT_EPSILON and use_alt_eps:\n",
    "                    v = ContextFreeGrammar.EPSILON\n",
    "                \n",
    "                self.rules[self.variables[-1]].append(v)\n",
    "    \n",
    "    def _get_eps_combinations(self, var, eps_var):\n",
    "        buffer = []\n",
    "        for value in self.rules[var]:\n",
    "            if eps_var in value:\n",
    "                buffer.extend(generate_eps_combinations(eps_var, value))\n",
    "        \n",
    "        return buffer\n",
    "    \n",
    "    def normalize(self):\n",
    "        # make a new start variable\n",
    "        self.add_rule('S_{0}->%s' % (self.start_variable))\n",
    "        eps = ContextFreeGrammar.EPSILON\n",
    "        \n",
    "        # remove ε-rules\n",
    "        found = False\n",
    "        first_round = True\n",
    "        \n",
    "        while found or first_round:\n",
    "            first_round = False\n",
    "            found = False\n",
    "            \n",
    "            for variable, values in self.rules.items():\n",
    "                if variable == self.start_variable:\n",
    "                    continue\n",
    "                if eps in values:\n",
    "                    found = True\n",
    "                    print(variable, 'has epsilon production')\n",
    "                    values.remove(eps)\n",
    "                    for k, v in self.rules.items():\n",
    "                        if k == variable:\n",
    "                            continue\n",
    "                        else:\n",
    "                            v.extend(self._get_eps_combinations(k, variable))\n",
    "                    break\n",
    "\n",
    "        # remove unit rules\n",
    "        for variable, values in self.rules.items():\n",
    "            found = []\n",
    "            for value in values:\n",
    "                if len(value) == 1 and value in self.variables:\n",
    "                    found.append(value)\n",
    "            for found_value in found:\n",
    "                values.remove(found_value)\n",
    "                if found_value != variable:\n",
    "                    values.extend(self.rules[found_value])\n",
    "                    \n",
    "        # add rules to satisfy A->BC\n",
    "        new_rules = set()\n",
    "        found = False\n",
    "        first_round = True\n",
    "        terminals = dict()\n",
    "        \n",
    "        while found or first_round:\n",
    "            found = False\n",
    "            res = False\n",
    "            \n",
    "            for variable, values in self.rules.items():\n",
    "                for value in values:\n",
    "                    res = generate_new_rules(new_rules, value)\n",
    "                    if res:\n",
    "                        found = res\n",
    "            if found:\n",
    "                new_rules_map = {key: 'U_{%d}' % (i + self._u_idx) for i, key in enumerate(list(new_rules))}\n",
    "                for key, value in terminals.items():\n",
    "                    if key in terminals and key in new_rules_map:\n",
    "                        new_rules_map[key] = value\n",
    "                for key, value in new_rules_map.items():\n",
    "                    if len(key) == 1 and match_lowercase.search(key):\n",
    "                        if key not in terminals:\n",
    "                            terminals[key] = value\n",
    "                \n",
    "                print(new_rules_map)\n",
    "                self._u_idx += len(new_rules_map)\n",
    "                for variable, values in self.rules.items():\n",
    "                    for i, value in enumerate(values):\n",
    "                        variables = match_variable.findall(value)\n",
    "                        \n",
    "                        if len(variables) >= 3:\n",
    "                            v = ''.join(variables[len(variables) - 2:])\n",
    "                            values[i] =  value.replace(v, new_rules_map[v])\n",
    "                        elif len(variables) == 2 and self._match_lowercase.search(value):\n",
    "                            for match in self._match_lowercase.finditer(value):\n",
    "                                values[i] = value.replace(match.group(), new_rules_map[match.group()])\n",
    "                \n",
    "                for value, variable in new_rules_map.items():\n",
    "                    self.add_rule('{}->{}'.format(variable, value))\n",
    "                \n",
    "                new_rules.clear()\n",
    "            \n",
    "            first_round = False\n",
    "        \n",
    "        # remove duplicates\n",
    "        for variable in self.variables:\n",
    "            self.rules[variable] = list(set(self.rules[variable]))\n",
    "            self.rules[variable].sort(key=lambda v: (len(v), v), reverse=True)\n",
    "    \n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.rules)\n",
    "    \n",
    "    def render_as_tex(self):\n",
    "        buffer = []\n",
    "        for variable in sorted(self.rules.keys()):\n",
    "            values = self.rules[variable]\n",
    "            buffer.append('$${} \\\\rightarrow {}$$\\n'.format(variable, '|'.join(values)))\n",
    "            buffer[-1].replace(ContextFreeGrammar.EPSILON, '\\\\epsilon')\n",
    "        return ''.join(buffer)\n",
    "    \n",
    "grammar = ContextFreeGrammar()\n",
    "grammar.add_rule('S->ASA|aB')\n",
    "grammar.add_rule('A->B|S')\n",
    "grammar.add_rule('B->b|e')\n",
    "grammar.normalize()\n",
    "Latex(grammar.render_as_tex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T has epsilon production\n",
      "U has epsilon production\n",
      "X has epsilon production\n",
      "R has epsilon production\n",
      "X has epsilon production\n",
      "Y has epsilon production\n",
      "Y has epsilon production\n",
      "V has epsilon production\n",
      "W has epsilon production\n",
      "W has epsilon production\n",
      "Z has epsilon production\n",
      "Z has epsilon production\n",
      "{'bZ': 'U_{0}', 'a': 'U_{1}', 'Wc': 'U_{2}', 'QT': 'U_{3}', 'c': 'U_{4}', 'bT': 'U_{5}', 'Vc': 'U_{6}', 'b': 'U_{7}', 'XT': 'U_{8}', 'Rb': 'U_{9}'}\n",
      "{'c': 'U_{4}', 'b': 'U_{7}', 'a': 'U_{1}'}\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$P \\rightarrow U_{1}T|a$$\n",
       "$$Q \\rightarrow U_{1}Q|QT$$\n",
       "$$R \\rightarrow U_{1}U_{9}$$\n",
       "$$S \\rightarrow U_{1}U_{8}|YU_{7}|YU_{5}|WU_{4}|U_{7}Z|U_{7}T|U_{1}X|U_{1}T|UU_{7}|UU_{4}|UU_{2}|UU_{0}|PU_{3}|PQ|c|b|a$$\n",
       "$$S_{0} \\rightarrow U_{1}U_{8}|YU_{7}|YU_{5}|WU_{4}|U_{7}Z|U_{7}T|U_{1}X|U_{1}T|UU_{7}|UU_{4}|UU_{2}|UU_{0}|PU_{3}|PQ|c|b|a$$\n",
       "$$T \\rightarrow U_{4}T$$\n",
       "$$U \\rightarrow U_{1}U$$\n",
       "$$U_{0} \\rightarrow U_{7}Z$$\n",
       "$$U_{1} \\rightarrow a$$\n",
       "$$U_{2} \\rightarrow WU_{4}$$\n",
       "$$U_{3} \\rightarrow QT$$\n",
       "$$U_{4} \\rightarrow c$$\n",
       "$$U_{5} \\rightarrow U_{7}T$$\n",
       "$$U_{6} \\rightarrow VU_{4}$$\n",
       "$$U_{7} \\rightarrow b$$\n",
       "$$U_{8} \\rightarrow XT$$\n",
       "$$U_{9} \\rightarrow RU_{7}$$\n",
       "$$V \\rightarrow U_{7}U_{6}$$\n",
       "$$W \\rightarrow U_{7}U_{6}|WU_{4}$$\n",
       "$$X \\rightarrow U_{1}U_{9}|U_{1}X$$\n",
       "$$Y \\rightarrow U_{1}U_{9}|YU_{7}$$\n",
       "$$Z \\rightarrow U_{7}U_{6}|U_{7}Z$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar = ContextFreeGrammar()\n",
    "grammar.add_rule('S->aXT|YbT|UbZ|UWc|PQT')\n",
    "grammar.add_rule('P->aT')\n",
    "grammar.add_rule('Q->QT|aQ')\n",
    "grammar.add_rule('T->cT|e')\n",
    "grammar.add_rule('U->aU|e')\n",
    "grammar.add_rule('X->aX|R|e')\n",
    "grammar.add_rule('R->aRb|e')\n",
    "grammar.add_rule('Y->Yb|R|e')\n",
    "grammar.add_rule('V->bVc|e')\n",
    "grammar.add_rule('W->Wc|V|e')\n",
    "grammar.add_rule('Z->bZ|V|e')\n",
    "grammar.normalize()\n",
    "Latex(grammar.render_as_tex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
